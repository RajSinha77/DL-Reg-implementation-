{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNeOoN33xT1CWJDaLVNOjNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajSinha77/DL-Reg-implementation-/blob/main/DLReg_vs_L2_on_Oxford_IIIT_Pet_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twurRWgPOsOF",
        "outputId": "9d8e4ded-54be-4176-804d-8d9744665045"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQchZ3QNMZwo",
        "outputId": "ee30f51f-0eb7-4203-b4db-2d1ac77a1de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['image', 'label', 'image_id', 'label_cat_dog'],\n",
            "        num_rows: 3680\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['image', 'label', 'image_id', 'label_cat_dog'],\n",
            "        num_rows: 3669\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define constants\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# Load the Oxford-IIIT Pet dataset from Hugging Face\n",
        "dataset = load_dataset('timm/oxford-iiit-pet')\n",
        "\n",
        "# Print the dataset structure to check available splits\n",
        "print(dataset)\n",
        "\n",
        "# Identify the correct splits\n",
        "train_split = 'train'\n",
        "test_split = 'test'\n",
        "\n",
        "# Custom dataset class to handle Hugging Face dataset\n",
        "class OxfordIIITPetDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        image = sample['image']\n",
        "        label = sample['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Transformations for the Oxford-IIIT Pet dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Lambda(lambda x: x.convert(\"RGB\") if x.mode != \"RGB\" else x),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Prepare the dataset and dataloaders\n",
        "train_val_dataset = OxfordIIITPetDataset(dataset[train_split], transform=transform)\n",
        "test_dataset = OxfordIIITPetDataset(dataset[test_split], transform=transform)\n",
        "\n",
        "# Split the train dataset into train and validation sets\n",
        "train_size = int((1 - VALIDATION_SPLIT) * len(train_val_dataset))\n",
        "val_size = len(train_val_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define the models with updated weights parameter\n",
        "resnet152 = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
        "densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify the final layers to fit Oxford-IIIT Pet classes (37 classes)\n",
        "num_ftrs_resnet = resnet152.fc.in_features\n",
        "resnet152.fc = nn.Linear(num_ftrs_resnet, 37)\n",
        "\n",
        "num_ftrs_densenet = densenet.classifier.in_features\n",
        "densenet.classifier = nn.Linear(num_ftrs_densenet, 37)\n",
        "\n",
        "# Define DL-Regularization\n",
        "class DLRegularization(nn.Module):\n",
        "    def __init__(self, model, lambda_reg=0.001):\n",
        "        super(DLRegularization, self).__init__()\n",
        "        self.model = model\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "    def dl_reg_loss(self, x, output):\n",
        "        batch_size = x.size(0)\n",
        "        Z = nn.Linear(x.view(batch_size, -1).size(1), output.size(1), bias=False).to(x.device)\n",
        "        reg_loss = torch.norm(Z(x.view(batch_size, -1)) - output) ** 2\n",
        "        return self.lambda_reg * reg_loss\n",
        "\n",
        "# Training function\n",
        "def train_model(model, criterion, optimizer, num_epochs=NUM_EPOCHS, dl_regularizer=None):\n",
        "    model.train()\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_corrects = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            if dl_regularizer:\n",
        "                reg_loss = dl_regularizer.dl_reg_loss(inputs, outputs)\n",
        "                loss += reg_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
        "        train_acc.append(epoch_acc.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_corrects = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_val_acc = val_corrects.double() / len(val_dataset)\n",
        "        val_acc.append(epoch_val_acc.item())\n",
        "\n",
        "        # Test\n",
        "        test_corrects = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_test_acc = test_corrects.double() / len(test_dataset)\n",
        "        test_acc.append(epoch_test_acc.item())\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Acc: {epoch_acc.item()}, Val Acc: {epoch_val_acc.item()}, Test Acc: {epoch_test_acc.item()}')\n",
        "\n",
        "    return train_acc, val_acc, test_acc\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Experiment with L2 regularization on ResNet\n",
        "resnet152 = resnet152.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet152.parameters(), lr=LEARNING_RATE, weight_decay=0.001)\n",
        "train_acc_l2_resnet, val_acc_l2_resnet, test_acc_l2_resnet = train_model(resnet152, criterion, optimizer)\n",
        "\n",
        "# Experiment with DL-Regularization on ResNet\n",
        "resnet152 = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
        "num_ftrs_resnet = resnet152.fc.in_features\n",
        "resnet152.fc = nn.Linear(num_ftrs_resnet, 37)\n",
        "resnet152 = resnet152.to(device)\n",
        "optimizer = optim.SGD(resnet152.parameters(), lr=LEARNING_RATE)\n",
        "dl_regularizer = DLRegularization(resnet152, lambda_reg=0.001)\n",
        "train_acc_dl_resnet, val_acc_dl_resnet, test_acc_dl_resnet = train_model(resnet152, criterion, optimizer, dl_regularizer=dl_regularizer)\n",
        "\n",
        "# Experiment with L2 regularization on DenseNet\n",
        "densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "num_ftrs_densenet = densenet.classifier.in_features\n",
        "densenet.classifier = nn.Linear(num_ftrs_densenet, 37)\n",
        "densenet = densenet.to(device)\n",
        "optimizer = optim.SGD(densenet.parameters(), lr=LEARNING_RATE, weight_decay=0.001)\n",
        "train_acc_l2_densenet, val_acc_l2_densenet, test_acc_l2_densenet = train_model(densenet, criterion, optimizer)\n",
        "\n",
        "# Experiment with DL-Regularization on DenseNet\n",
        "densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "num_ftrs_densenet = densenet.classifier.in_features\n",
        "densenet.classifier = nn.Linear(num_ftrs_densenet, 37)\n",
        "densenet = densenet.to(device)\n",
        "optimizer = optim.SGD(densenet.parameters(), lr=LEARNING_RATE)\n",
        "dl_regularizer = DLRegularization(densenet, lambda_reg=0.001)\n",
        "train_acc_dl_densenet, val_acc_dl_densenet, test_acc_dl_densenet = train_model(densenet, criterion, optimizer, dl_regularizer=dl_regularizer)\n",
        "\n",
        "# Plotting the results\n",
        "epochs = range(1, NUM_EPOCHS + 1)\n",
        "plt.figure(figsize=(18, 18))\n",
        "\n",
        "plt.subplot(3, 2, 1)\n",
        "plt.plot(epochs, train_acc_l2_resnet, label='L2 Regularization')\n",
        "plt.plot(epochs, train_acc_dl_resnet, label='DL-Regularization')\n",
        "plt.title('ResNet Training Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 2, 2)\n",
        "plt.plot(epochs, val_acc_l2_resnet, label='L2 Regularization')\n",
        "plt.plot(epochs, val_acc_dl_resnet, label='DL-Regularization')\n",
        "plt.title('ResNet Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 2, 3)\n",
        "plt.plot(epochs, train_acc_l2_densenet, label='L2 Regularization')\n",
        "plt.plot(epochs, train_acc_dl_densenet, label='DL-Regularization')\n",
        "plt.title('DenseNet Training Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 2, 4)\n",
        "plt.plot(epochs, val_acc_l2_densenet, label='L2 Regularization')\n",
        "plt.plot(epochs, val_acc_dl_densenet, label='DL-Regularization')\n",
        "plt.title('DenseNet Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 2, 5)\n",
        "plt.plot(epochs, test_acc_l2_resnet, label='L2 Regularization')\n",
        "plt.plot(epochs, test_acc_dl_resnet, label='DL-Regularization')\n",
        "plt.title('ResNet Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 2, 6)\n",
        "plt.plot(epochs, test_acc_l2_densenet, label='L2 Regularization')\n",
        "plt.plot(epochs, test_acc_dl_densenet, label='DL-Regularization')\n",
        "plt.title('DenseNet Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save png image to desktop\n",
        "\n",
        "plt.savefig(r'C:\\Users\\rajsi\\OneDrive\\Desktop\\FAU Courses\\SS24\\intro cml\\resnet_densenet_comparison.png')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4ORc2v9eAGTi",
        "outputId": "16c9c4da-2f4b-4f65-f1aa-5536a5b49b96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}